# 迁移学习和传统机器学习在手写数字识别问题上的性能差异分析报告

## 工作概述：

* 运用matlab读取手写数字识别数据集，对其分别进行传统机器学习建模和迁移学习建模。对比分析两者性能差异。

## 工作计划：

1. 准备数据集，进行前期的数据清洗工作，使其转化成机器学习算法能够输入的数据结构。

2. 用传统机器学习对其进行建模（svm、logistics回归、决策树、xgboost、lightgbm等视matlab能够提供的算法包决定）

3. 用CNN网络进行迁移学习。原网络架构和参数视matlab能够提供的网络参数决定。

4. 对比分析两者训练时间、训练误差等性能差异。

5. 得出结论：在手写数字识别问题中哪种方法可以获得合适的需求的性能。


## 工作内容

1. 选择数据集：matlab自带数据集：DigitDataset
    - 数据规模：0-9各1000个，综合10000个

2. 初始数据为28x28x1的图片，为了适应后续Alexnet的输入，将其变换到127下27*3的大小

3. 训练LettersClassificationNet，记录训练数据

4. 训练AlexNet，记录训练数据

5. 将输入数据二值化并展开为一行，每张图片成为1x784的特征向量

6. 训练svm，尝试不同核函数和超参数，记录训练数据

7. 分析对比两种数据，得出结论

## 实验结论

- 手写数字图像识别用迁移学习将会有更加好的准确率

- 如果任务类型是图像、语音、翻译等已经有成熟大型开源深度神经网络的情况下，追求准确率应该使用迁移学习

- 当任务没有成熟架构和成熟网络或者追求可解释性（指导业务改进）时，还是可以用传统机器学习，通常也可以取得不错的结果


## 实验收获

- 迁移学习不是很受原网络架构影响，本次训练了matlab自带的LettersClassificationNet(以下简称LCnet)和大名鼎鼎Alexnet，两者网络复杂程度完全不是一个量级，LCnet专门是训练的用来分类字母和数字的28x28x1输入的网络，而Alexnet是在2012Imagenet竞赛大放异彩的可以分类1000种图片的大型网络，输入为127x127x3但是从分类正确率上来说,两者都非常出色，Alexnet甚至可以达到100%，由此可见只要原网络起初训练的任务相同（视觉、语音、翻译等大类），由复杂任务网络迁移学习简单任务是非常好的方法。

- 越复杂的网络因为硬件条件的限制（GPU缓存大小）可以承受的mini-batchsize上界越小，但是也不容易不收敛。LCnet在mini-batchsize=16时就不能收敛，然而Alexnetzai 在mini-batchsize=16仍然有很好的性能

- mini-batchsize参数很能够影响系统训练后的性能，太小则会在最优解附近转圈不能收敛到一个最优值，太大则会超出GPU内存限制

- mini-batchsize越小，单次iteration越快，但是训练方差变大，需要综合GPU缓存大小、神经网络大小、目标精度等信息选择合适参数

- 传统机器学习能力有限，不一定能非常好的拟合数据，而且需要很多繁琐的前期特征提取工作，在追求预测精度的要求下应该首先考虑迁移学习

- 从训练时间上来说，传统机器学习显然更甚一筹，但是这是针对单次训练而言。在参数寻优过程中，迁移学习计算空间显然远远小于传统机器学习（原网络架构和最佳参数已经给定，仅仅需要小幅微调即可）。而且可以看到即使是最优的SVM参数训练出来的模型，准确率也远远达不到普通参数通过LCnet迁移学习训练出来的模型（Alextnet更加比不上了）

- 在现有GPU加速下，其实迁移学习计算复杂度已经到了可以接受的范围内（Alextnet 在Geforce 940m这种很普通的计算硬件加速下迁移学习最后三层网络只要15min左右）。
  
## 实验反思和改进策略：

### 改进策略

- 本次实验迁移学习限制于硬件设备和时间因素只调整了mini-batchsize参数，可以对其他训练参数进行分析

- 传统机器学习选择了SVM进行训练，在特征工程方面可以进行更加细致的工作
    - 考虑用滤波器对图片进行卷积构建特征
    - 图片处理上对于二值化的阈值进行好好调整